{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers pillow gTTS torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from google.colab import files\n",
    "from IPython.display import Audio, display\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base').to(device)\n",
    "\n",
    "def caption_image(pil_img):\n",
    "    inputs = processor(images=pil_img, return_tensors='pt').to(device)\n",
    "    out = model.generate(**inputs, max_new_tokens=20)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "name = list(uploaded.keys())[0]\n",
    "img = Image.open(name).convert('RGB')\n",
    "display(img)\n",
    "\n",
    "cap = caption_image(img)\n",
    "print('Caption:', cap)\n",
    "\n",
    "tts = gTTS(cap)\n",
    "tts.save('caption.mp3')\n",
    "Audio('caption.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self, nz=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(nz,256), nn.ReLU(),\n",
    "            nn.Linear(256,28*28), nn.Tanh()\n",
    "        )\n",
    "    def forward(self,z): return self.net(z).view(-1,1,28,28)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28,256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
    "mnist = torchvision.datasets.MNIST(root='.', download=True, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(mnist, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen,disc = G().to(device), D().to(device)\n",
    "optG = optim.Adam(gen.parameters(), lr=0.0005)\n",
    "optD = optim.Adam(disc.parameters(), lr=0.0005)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "batch,_ = next(iter(loader))\n",
    "batch = batch.to(device)\n",
    "B = batch.size(0)\n",
    "\n",
    "z = torch.randn(B,64).to(device)\n",
    "fake = gen(z)\n",
    "real_loss = loss_fn(disc(batch), torch.ones(B,1).to(device))\n",
    "fake_loss = loss_fn(disc(fake.detach()), torch.zeros(B,1).to(device))\n",
    "d_loss = real_loss + fake_loss\n",
    "optD.zero_grad(); d_loss.backward(); optD.step()\n",
    "\n",
    "output = disc(fake)\n",
    "g_loss = loss_fn(output, torch.ones(B,1).to(device))\n",
    "optG.zero_grad(); g_loss.backward(); optG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(16,64).to(device)\n",
    "samples = gen(z).cpu()\n",
    "samples = (samples + 1)/2\n",
    "grid = make_grid(samples, nrow=4)\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Flatten(), nn.Linear(28*28,128), nn.ReLU())\n",
    "        self.mu = nn.Linear(128,20)\n",
    "        self.logvar = nn.Linear(128,20)\n",
    "        self.dec = nn.Sequential(nn.Linear(20,128), nn.ReLU(), nn.Linear(128,28*28), nn.Sigmoid())\n",
    "    def encode(self,x):\n",
    "        h=self.enc(x); return self.mu(h), self.logvar(h)\n",
    "    def reparam(self,mu,lv): return mu + torch.randn_like(mu)*torch.exp(0.5*lv)\n",
    "    def forward(self,x):\n",
    "        mu,lv=self.encode(x); z=self.reparam(mu,lv)\n",
    "        return self.dec(z).view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "batch,_ = next(iter(loader))\n",
    "batch=batch.to(device)\n",
    "recon = vae(batch)\n",
    "loss = ((batch-recon)**2).mean()\n",
    "opt.zero_grad(); loss.backward(); opt.step()\n",
    "print('VAE trained once, loss =', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(16,20).to(device)\n",
    "with torch.no_grad():\n",
    "    gen_imgs = vae.dec(z).view(-1,1,28,28)\n",
    "\n",
    "grid = make_grid(gen_imgs, nrow=4)\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
